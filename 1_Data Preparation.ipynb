{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\aashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, map all uppercased words to lowercase. \n",
    "\n",
    "Then, create a list of words with frequency less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = brown.words()\n",
    "wordlist_lowercased = [i.lower() for i in word_list]  # Mapping uppercased words to lowercase\n",
    "c = dict(Counter(wordlist_lowercased)) # Dictionary containing frequency of lowercased word list\n",
    "lf_words_dict = dict((key, value) for key, value in c.items() if value<4)  # Removing words with frequency <=3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word list by removing words with frequency less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for loop is:779.5225172042847\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for key, value in lf_words_dict.items():\n",
    "    for i in range(value):\n",
    "        wordlist_lowercased.remove(key)  # Converting to lowercase\n",
    "processed_wordlist = wordlist_lowercased.copy()\n",
    "stop = time.time()\n",
    "print('Time for loop is:' + str(stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the Brown Corpus such that the first 800,000 words are used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 161,192 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in training set are: 800000\n",
      "Number of words in validation set are: 200000\n",
      "Number of words in testing set are: 113024\n"
     ]
    }
   ],
   "source": [
    "training_words = processed_wordlist[0:800000]\n",
    "validation_words = processed_wordlist[800000:1000000]\n",
    "testing_words = processed_wordlist[1000000:len(processed_wordlist)]\n",
    "\n",
    "print('Number of words in training set are: '+ str(len(training_words)))\n",
    "print('Number of words in validation set are: '+ str(len(validation_words)))\n",
    "print('Number of words in testing set are: '+ str(len(testing_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sequences of N+1 words from training set, validation set and testing set wherein, the first 10 / 50 / 100 / 300 words are input sequences and the 11th word is the output word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating input sequences of length 'N'\n",
    "\n",
    "N = 300\n",
    "training_sequences = []\n",
    "validation_sequences = []\n",
    "testing_sequences = []\n",
    "\n",
    "for i in range(N, len(training_words)):\n",
    "    training_sequences.append(training_words[i-N:i+1])\n",
    "\n",
    "for j in range(N, len(validation_words)):\n",
    "    validation_sequences.append(validation_words[j-N:j+1])\n",
    "\n",
    "for k in range(N, len(testing_words)):\n",
    "    testing_sequences.append(testing_words[k-N:k+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences are: 799700\n",
      "Number of validation sequences are: 199700\n",
      "Number of testing sequences are: 112724\n"
     ]
    }
   ],
   "source": [
    "print('Number of training sequences are: '+ str(len(training_sequences)))\n",
    "print('Number of validation sequences are: '+ str(len(validation_sequences)))\n",
    "print('Number of testing sequences are: '+ str(len(testing_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing word sequences into csv files, by name, 'training_sequences.csv', 'validation_sequences.csv', 'testing_sequences.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"word_sequences\")\n",
    "\n",
    "df_train_seq = pd.DataFrame(training_sequences)\n",
    "df_val_seq = pd.DataFrame(validation_sequences)\n",
    "df_test_seq = pd.DataFrame(testing_sequences)\n",
    "\n",
    "df_train_seq.to_csv('word_sequences/training_sequences.csv')\n",
    "df_val_seq.to_csv('word_sequences/validation_sequences.csv')\n",
    "df_test_seq.to_csv('word_sequences/testing_sequences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing dictionary with serial number, words and corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(processed_wordlist))  # List of lowercased words\n",
    "snum = list(range(len(unique_words))) # Serial Number for unique words\n",
    "c = dict(Counter(processed_wordlist))\n",
    "data = {'Serial Number': snum, 'Word' : list(c.keys()), 'Frequency' : list(c.values())}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16689\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an Embedding Layer to learn the representation of words. \n",
    "\n",
    "The word embedding layer expects input sequences to be comprised of integers.\n",
    "\n",
    "We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping.\n",
    "\n",
    "We will write each of these sequences into csv files, by name, 'encoded_training.csv', 'encoded_validation.csv', 'encoded_testing.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('encoded_sequences')\n",
    "word_dictionary = dict(zip(list(c.keys()), snum))\n",
    "encoded_training = []\n",
    "encoded_validation = []\n",
    "encoded_testing = []\n",
    "\n",
    "for i in range(len(training_sequences)):\n",
    "    temp1 = []\n",
    "    for seq1 in range(N + 1):\n",
    "        word = training_sequences[i][seq1]\n",
    "        temp1.append(word_dictionary[word])\n",
    "    encoded_training.append(temp1) \n",
    "        \n",
    "for j in range(len(validation_sequences)):\n",
    "    temp2 = []\n",
    "    for seq2 in range(N + 1):\n",
    "        word = training_sequences[j][seq2]\n",
    "        temp2.append(word_dictionary[word])\n",
    "    encoded_validation.append(temp2)\n",
    "    \n",
    "for k in range(len(testing_sequences)):\n",
    "    temp3 = []\n",
    "    for seq3 in range(N + 1):\n",
    "        word = training_sequences[k][seq3]\n",
    "        temp3.append(word_dictionary[word])\n",
    "    encoded_testing.append(temp3)\n",
    "\n",
    "df_enc_train = pd.DataFrame(encoded_training)\n",
    "df_enc_val = pd.DataFrame(encoded_validation)\n",
    "df_enc_test = pd.DataFrame(encoded_testing)\n",
    "\n",
    "df_enc_train_20pc = pd.DataFrame(encoded_training).sample(frac=0.2)\n",
    "df_enc_val_20pc = pd.DataFrame(encoded_validation).sample(frac=0.2)\n",
    "df_enc_test_20pc = pd.DataFrame(encoded_testing).sample(frac=0.2)\n",
    "\n",
    "df_enc_train.to_csv('encoded_sequences/encoded_training.csv')\n",
    "df_enc_val.to_csv('encoded_sequences/encoded_validation.csv')\n",
    "df_enc_test.to_csv('encoded_sequences/encoded_testing.csv')\n",
    "\n",
    "df_enc_train_20pc.to_csv('encoded_sequences/encoded_training_20pc.csv')\n",
    "df_enc_val_20pc.to_csv('encoded_sequences/encoded_validation_20pc.csv')\n",
    "df_enc_test_20pc.to_csv('encoded_sequences/encoded_testing_20pc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
